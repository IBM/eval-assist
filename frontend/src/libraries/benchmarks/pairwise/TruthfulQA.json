{
    "name": "TruthfulQA",
    "description": "TruthfulQA is a dataset to measure whether a language model is truthful in generating answers to questions. The dataset comprises 38 categories, including health, law, finance and politics. The language model has to decide which response is the truthful response.",
    "link": "https://huggingface.co/datasets/truthfulqa/truthful_qa",
    "type": "pairwise",
    "dataset": {
        "name": "TruthfulQA",
        "description": "loren ipsum"
    },
    "tags": [],
    "criteriaBenchmarks": [
        {
            "name": "Truthfulness",
            "evaluatorBenchmarks": [
                {
                    "evaluator_id": "meta-llama/llama-3-70b-instruct",
                    "laaj_version": "v0.0.7-alpha",
                    "results": {
                        "agreement": 0.918,
                        "p_bias": 0.03,
                        "pearson": 0.847
                    }
                },
                {
                    "evaluator_id": "mistralai/mixtral-8x7b-instruct-v01",
                    "laaj_version": "v0.0.7-alpha",
                    "results": {
                        "agreement": 0.878,
                        "p_bias": 0.02,
                        "pearson": 0.755
                    }
                },
                {
                    "evaluator_id": "kaist-ai/prometheus-8x7b-v2",
                    "laaj_version": "v0.0.7-alpha",
                    "results": {
                        "agreement": 0.844,
                        "p_bias": 0.23,
                        "pearson": 0.687
                    }
                },
                {
                    "evaluator_id": "meta-llama/llama-3-8b-instruct",
                    "laaj_version": "v0.0.7-alpha",
                    "results": {
                        "agreement": 0.804,
                        "p_bias": 0.03,
                        "pearson": 0.61
                    }
                }
            ]
        }
    ]
}